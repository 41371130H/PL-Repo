{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa/LPkHLdUYStt0s/EchZT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371130H/PL-Repo/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Š PTT ç†±è©åˆ†æèˆ‡æ‘˜è¦ç³»çµ± (Gradio ä»‹é¢)\n",
        "\n",
        "*æ­¡è¿ä½¿ç”¨æœ¬ç³»çµ±ï¼é€™æ˜¯ä¸€å€‹å°ˆç‚ºåˆ†æ PTT æ—¥åŠ‡ç‰ˆ (Japandrama) è¼¿æƒ…è€Œè¨­è¨ˆçš„è‡ªå‹•åŒ–å·¥å…·ã€‚*\n",
        "\n",
        "*æ‚¨å¯ä»¥é€éé€™å€‹ç¶²é ä»‹é¢ï¼Œä¸€éµå•Ÿå‹•çˆ¬èŸ²æŠ“å–æœ€æ–°æ–‡ç« ã€åˆ†æç†±é–€é—œéµå­—ï¼Œä¸¦åˆ©ç”¨ Gemini è‡ªå‹•ç”Ÿæˆå°ˆæ¥­çš„è¶¨å‹¢æ‘˜è¦ã€‚*\n",
        "\n",
        "Google Sheet é€£çµï¼šhttps://docs.google.com/spreadsheets/d/1mw8T-_jsLFGHZcq1EpreAZFjdY4TXoH-uPOtGyBKQW4/edit?usp=sharing\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ è‡ªå‹•åŒ–æµç¨‹åŸ·è¡Œ (ä¸»è¦åŠŸèƒ½)\n",
        "\n",
        "é€™æ˜¯æœ¬ç³»çµ±çš„æ ¸å¿ƒåŠŸèƒ½é ç±¤ã€‚\n",
        "\n",
        "### **1. åƒæ•¸è¼¸å…¥**\n",
        "\n",
        "åœ¨å•Ÿå‹•æµç¨‹å‰ï¼Œæ‚¨éœ€è¦è¨­å®šå…©å€‹åƒæ•¸ï¼š\n",
        "\n",
        "* **è¦çˆ¬å–çš„é æ•¸**ï¼šè¨­å®šè¦å¾ PTT æ—¥åŠ‡ç‰ˆæŠ“å–å¤šå°‘é çš„æ–‡ç« åˆ—è¡¨ (ä¾‹å¦‚è¼¸å…¥ 10ï¼Œä»£è¡¨æŠ“å–æœ€æ–°çš„ 10 é )ã€‚\n",
        "* **è¦çµ±è¨ˆçš„ Top N ç†±è©æ•¸é‡**ï¼šè¨­å®šæ‚¨å¸Œæœ›ç³»çµ±åˆ†æå‡ºå‰å¹¾åçš„ç†±é–€é—œéµå­— (ä¾‹å¦‚è¼¸å…¥ 20ï¼Œä»£è¡¨æ‰¾å‡º Top 20 ç†±è©)ã€‚\n",
        "* **ğŸš€ ä¸€éµå•Ÿå‹•è‡ªå‹•åŒ–æµç¨‹**ï¼šè¨­å®šå¥½åƒæ•¸å¾Œï¼Œé»æ“Šæ­¤æŒ‰éˆ•å³å¯é–‹å§‹åŸ·è¡Œã€‚ç³»çµ±æœƒè‡ªå‹•å®Œæˆæ‰€æœ‰åˆ†ææ­¥é©Ÿã€‚\n",
        "\n",
        "### **2. åˆ†æçµæœ (å­åˆ†é )**\n",
        "\n",
        "åˆ†æé–‹å§‹å¾Œï¼Œæ‚¨å¯ä»¥åœ¨ä¸‹æ–¹çš„ä¸‰å€‹å­åˆ†é ä¸­æŸ¥çœ‹å³æ™‚é€²åº¦èˆ‡æœ€çµ‚çµæœï¼š\n",
        "\n",
        "#### **ğŸ› ï¸ æŠ€è¡“æ—¥èªŒèˆ‡è¼¸å‡ºç´°ç¯€**\n",
        "*é€™å€‹åˆ†é ç”¨ä¾†è¿½è¹¤ç³»çµ±ç›®å‰çš„åŸ·è¡Œç‹€æ…‹ã€‚*\n",
        "* **è©³ç´°æµç¨‹æ—¥èªŒ**ï¼šç³»çµ±æœƒå³æ™‚æ›´æ–°ç›®å‰çš„é€²åº¦ï¼Œå¦‚æœæµç¨‹ä¸å¹¸ç™¼ç”ŸéŒ¯èª¤ï¼Œè©³ç´°çš„éŒ¯èª¤è¨Šæ¯ä¹Ÿæœƒé¡¯ç¤ºåœ¨æ­¤è™•ã€‚\n",
        "\n",
        "#### **ğŸ•¸ï¸ çˆ¬å–ä¹‹ç¶²ç«™**\n",
        "*é€™å€‹åˆ†é ç”¨ä¾†æª¢è¦–åŸå§‹è³‡æ–™ã€‚*\n",
        "* **è³‡æ–™ä¾†æº**ï¼šé¡¯ç¤ºç›®å‰çˆ¬å–çš„çœ‹æ¿ (ä¾‹å¦‚ï¼šPTT æ—¥åŠ‡ç‰ˆ)ã€‚\n",
        "* **ç¶²ç«™é€£çµ**ï¼šé»æ“Šè¡¨æ ¼ä¸­çš„æ–‡ç« ï¼Œå¯åœ¨ä¸Šæ–¹é¡¯ç¤ºæ¨™é¡Œèˆ‡é€£çµï¼Œä¸¦æ–¼æ–°åˆ†é é–‹å•ŸåŸå§‹ PTT æ–‡ç« ã€‚\n",
        "* **çˆ¬å–æ–‡ç« åˆ—è¡¨ (åŸå§‹è³‡æ–™)**ï¼šä»¥è¡¨æ ¼å½¢å¼é¡¯ç¤ºç³»çµ±å¾ PTT æŠ“å–åˆ°çš„æ‰€æœ‰æ–‡ç« æ¨™é¡Œã€ä½œè€…ã€æ—¥æœŸèˆ‡é€£çµã€‚\n",
        "\n",
        "#### **âœ… æœ€çµ‚çµæœ**\n",
        "*é€™æ˜¯æ‚¨æœ€éœ€è¦é—œæ³¨çš„åˆ†é ã€‚* é€™è£¡æœƒé¡¯ç¤ºåˆ†æçš„æœ€çµ‚ç”¢å‡ºï¼š\n",
        "* **ğŸ¤– Gemini æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–**ï¼šç”± AI æ ¹æ“šç†±è©ç”Ÿæˆçš„å°ˆæ¥­åˆ†æå ±å‘Šï¼ŒåŒ…å«è¶¨å‹¢æ´å¯Ÿèˆ‡çµè«–ã€‚\n",
        "* **ğŸ“ˆ Top N ç†±è©è¦–è¦ºåŒ–åœ–è¡¨**ï¼šå°‡å‰ N åçš„ç†±é–€é—œéµå­—ä»¥æ°´å¹³é•·æ¢åœ–é¡¯ç¤ºï¼Œè®“æ‚¨ä¸€çœ¼çœ‹å‡ºå“ªäº›è©å½™æœ€ç†±é–€ã€‚\n",
        "* **ğŸ“ˆ Top N ç†±è©çµ±è¨ˆçµæœ (Data)**ï¼šé¡¯ç¤ºåœ–è¡¨çš„åŸå§‹æ•¸æ“šï¼ŒåŒ…å«é—œéµå­—åŠå…¶ TF-IDF æ¬Šé‡åˆ†æ•¸ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¬ Jieba åˆ†è©æ¨¡å¼æ¸¬è©¦ (è¼”åŠ©å·¥å…·)\n",
        "\n",
        "é€™æ˜¯ä¸€å€‹ç¨ç«‹çš„å°å·¥å…·ï¼Œç”¨ä¾†æ¸¬è©¦ä¸­æ–‡åˆ†è©çš„æ•ˆæœã€‚\n",
        "\n",
        "1.  **è¼¸å…¥ä¸­æ–‡å¥å­**ï¼šåœ¨è¼¸å…¥æ¡†ä¸­è²¼ä¸Šæ‚¨æƒ³æ¸¬è©¦çš„ä»»ä½•ä¸­æ–‡å¥å­ã€‚\n",
        "2.  **ğŸ” åŸ·è¡Œåˆ†æ**ï¼šé»æ“ŠæŒ‰éˆ•ã€‚\n",
        "3.  **æŸ¥çœ‹çµæœ**ï¼šä¸‹æ–¹æœƒç«‹å³é¡¯ç¤ºå››ç¨®ä¸åŒçš„åˆ†è©æ¨¡å¼çµæœï¼Œå¹«åŠ©æ‚¨äº†è§£ä¸åŒæ¼”ç®—æ³•æ˜¯å¦‚ä½•åˆ‡åˆ†è©å½™çš„ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## â˜ï¸ é›²ç«¯è‡ªå‹•å‚™ä»½ (å¾Œå°åŠŸèƒ½)\n",
        "\n",
        "é™¤äº†åœ¨ä»‹é¢ä¸Šé¡¯ç¤ºçµæœå¤–ï¼Œç•¶æ‚¨æŒ‰ä¸‹ã€Œä¸€éµå•Ÿå‹•ã€æ™‚ï¼Œç³»çµ±ä¹Ÿæœƒåœ¨èƒŒæ™¯åŸ·è¡Œï¼š\n",
        "\n",
        "1.  å°‡ã€Œçˆ¬å–æ–‡ç« åˆ—è¡¨ (åŸå§‹è³‡æ–™)ã€å®Œæ•´å‚™ä»½åˆ°æŒ‡å®šçš„ **Google Sheets** å·¥ä½œè¡¨ (\"PTTæ–‡ç« åˆ—è¡¨\")ã€‚\n",
        "2.  å°‡ã€ŒTop N ç†±è©çµ±è¨ˆçµæœã€å‚™ä»½åˆ°å¦ä¸€å¼µ **Google Sheets** å·¥ä½œè¡¨ (\"ç†±è©çµ±è¨ˆ\")ã€‚\n"
      ],
      "metadata": {
        "id": "HejDpicwru0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-UNeO8r92_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ce7cdc-5323-4163-f5af-10d640c984fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.26.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.185.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gspread google-generativeai gradio requests beautifulsoup4 jieba pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Google æˆæ¬Š\n",
        "# ================================\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# ================================\n",
        "# 2. æª¢æŸ¥ Gemini API é‡‘é‘°\n",
        "# ================================\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    api_key = userdata.get(\"gemini\")\n",
        "except Exception as e:\n",
        "    print(f\"ğŸš¨ è®€å–é‡‘é‘°æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")"
      ],
      "metadata": {
        "id": "uvd8o1S3A_m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import pytz\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import jieba  # ä½¿ç”¨é‡å°ç¹é«”ä¸­æ–‡å„ªåŒ–çš„ jieba\n",
        "import jieba.analyse\n",
        "import jieba.posseg as pseg\n",
        "import google.generativeai as genai\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import time\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import traceback"
      ],
      "metadata": {
        "id": "EYSAREYqBQJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æœ‰æ”¹æ¨¡å‹"
      ],
      "metadata": {
        "id": "EdbrKbRXB6xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Sheets èˆ‡ Gemini æˆæ¬Šèˆ‡è¨­å®š\n",
        "try:\n",
        "    # --- Google Sheets ---\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1mw8T-_jsLFGHZcq1EpreAZFjdY4TXoH-uPOtGyBKQW4/edit?usp=sharing\"\n",
        "    gsheets = gc.open_by_url(SPREADSHEET_URL)\n",
        "\n",
        "    # --- Gemini API ---\n",
        "    GEMINI_API_KEY = userdata.get(\"gemini\")\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ğŸš¨ æˆæ¬Šæˆ–è¨­å®šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")"
      ],
      "metadata": {
        "id": "7K1Oc3ByBdqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.1 PTT çˆ¬èŸ² (Helper å‡½å¼) ---\n",
        "def get_previous_page_url(soup):\n",
        "    \"\"\"ç²å–ä¸Šä¸€é  (æ›´èˆŠçš„æ–‡ç« ) çš„é€£çµ\"\"\"\n",
        "    paging_div = soup.find('div', class_='btn-group btn-group-paging')\n",
        "    if paging_div:\n",
        "        prev_button = paging_div.find_all('a')[1]\n",
        "        if 'href' in prev_button.attrs:\n",
        "            return \"https://www.ptt.cc\" + prev_button['href']\n",
        "    return None\n",
        "\n",
        "def extract_index_from_url(url):\n",
        "    \"\"\"å¾ PTT ç¶²å€ä¸­è§£æå‡ºé ç¢¼\"\"\"\n",
        "    try:\n",
        "        # ä½¿ç”¨æ­£è¦è¡¨é”å¼åŒ¹é… index å¾Œé¢çš„æ•¸å­—\n",
        "        match = re.search(r'index(\\d+)\\.html', url)\n",
        "        if match:\n",
        "             return int(match.group(1))\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# --- 3.1 PTT çˆ¬èŸ² (ä¸»è¦å‡½å¼) ---\n",
        "def scrape_ptt_japandrama(pages_to_fetch, log_output):\n",
        "    \"\"\"çˆ¬å– PTT Japandrama ç‰ˆæŒ‡å®šé æ•¸çš„æ–‡ç« åˆ—è¡¨ï¼Œä¸¦è¨˜éŒ„è©³ç´°æ—¥èªŒ\"\"\"\n",
        "    BOARD_NAME = \"Japandrama\"\n",
        "    START_URL = f\"https://www.ptt.cc/bbs/{BOARD_NAME}/index.html\"\n",
        "    BASE_URL = f\"https://www.ptt.cc/bbs/{BOARD_NAME}/index\"\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.post(\"https://www.ptt.cc/ask/over18\", data={\"yes\": \"yes\"})\n",
        "\n",
        "    all_data_list = []\n",
        "    log_output.append(f\"--- 1. çˆ¬èŸ²æ—¥èªŒ ---\")\n",
        "    log_output.append(f\"ç›®æ¨™çœ‹æ¿: {BOARD_NAME} | çˆ¬å–é æ•¸: {pages_to_fetch}\")\n",
        "\n",
        "    try:\n",
        "        # 1. å–å¾—æœ€æ–°çš„é ç¢¼\n",
        "        r = session.get(START_URL, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        prev_page_url = get_previous_page_url(soup)\n",
        "\n",
        "        if not prev_page_url:\n",
        "            log_output.append(\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° 'ä¸Šä¸€é ' é€£çµï¼Œç„¡æ³•å®šä½æœ€æ–°é ç¢¼ã€‚\")\n",
        "            return pd.DataFrame(), log_output\n",
        "\n",
        "        start_index = extract_index_from_url(prev_page_url)\n",
        "        if start_index is None:\n",
        "            log_output.append(\"âŒ éŒ¯èª¤ï¼šç„¡æ³•å¾é€£çµä¸­è§£æå‡ºèµ·å§‹é ç¢¼ã€‚\")\n",
        "            return pd.DataFrame(), log_output\n",
        "\n",
        "        log_output.append(f\"èµ·å§‹é ç¢¼ (å‰ä¸€é ): {start_index}ã€‚å°‡å¾ {start_index} éæ¸›çˆ¬å–ã€‚\")\n",
        "\n",
        "        # å…ˆçˆ¬å–æœ€æ–°ä¸€é \n",
        "        log_output.append(f\" -> çˆ¬å–é é¢ (æœ€æ–°): {START_URL}\")\n",
        "        articles = soup.find_all(\"div\", class_=\"r-ent\")\n",
        "        for art in articles:\n",
        "          title_tag = art.find(\"div\", class_=\"title\").find(\"a\")\n",
        "          if title_tag and \"[å…¬å‘Š]\" not in title_tag.text:\n",
        "            title = title_tag.text\n",
        "            link = \"https://www.ptt.cc\" + title_tag[\"href\"]\n",
        "            author = art.find(\"div\", class_=\"author\").text\n",
        "            date_str = art.find(\"div\", class_='date').text.strip()\n",
        "            all_data_list.append({\n",
        "              \"æ—¥æœŸ\": date_str, \"ä½œè€…\": author, \"æ¨™é¡Œ\": title, \"é€£çµ\": link\n",
        "            })\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "        # 2. è¿´åœˆçˆ¬å–\n",
        "        pages_remaining = pages_to_fetch - 1\n",
        "        if pages_remaining > 0:\n",
        "          stop_index = start_index - pages_remaining\n",
        "          for index in range(start_index, stop_index, -1):\n",
        "              url = f\"{BASE_URL}{index}.html\"\n",
        "              log_output.append(f\" -> çˆ¬å–é é¢ {index}:{url}\")\n",
        "\n",
        "              try:\n",
        "                  r_page = session.get(url, timeout=10)\n",
        "                  if r_page.status_code != 200:\n",
        "                      log_output.append(f\"   âš ï¸ è·³éé é¢ (ç‹€æ…‹ç¢¼: {r_page.status_code})\")\n",
        "                      continue\n",
        "\n",
        "                  soup_page = BeautifulSoup(r_page.text, \"html.parser\")\n",
        "                  articles = soup_page.find_all(\"div\", class_=\"r-ent\")\n",
        "\n",
        "                  for art in articles:\n",
        "                      title_tag = art.find(\"div\", class_=\"title\").find(\"a\")\n",
        "                      if title_tag and \"[å…¬å‘Š]\" not in title_tag.text:\n",
        "                          title = title_tag.text\n",
        "                          link = \"https://www.ptt.cc\" + title_tag[\"href\"]\n",
        "                          author = art.find(\"div\", class_=\"author\").text\n",
        "                          date_str = art.find(\"div\", class_='date').text.strip()\n",
        "                          all_data_list.append({\n",
        "                              \"æ—¥æœŸ\": date_str, \"ä½œè€…\": author, \"æ¨™é¡Œ\": title, \"é€£çµ\": link\n",
        "                          })\n",
        "\n",
        "                  time.sleep(0.1) # ç¸®çŸ­å»¶é²ä»¥åŠ é€Ÿç¤ºç¯„\n",
        "\n",
        "              except requests.exceptions.RequestException as e:\n",
        "                  log_output.append(f\"   âŒ çˆ¬å–å¤±æ•—: {e}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_output.append(f\"âŒ çˆ¬èŸ²èµ·å§‹è«‹æ±‚å¤±æ•—ï¼š{e}\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "    df = pd.DataFrame(all_data_list)\n",
        "    log_output.append(f\"âœ… çˆ¬èŸ²çµæŸã€‚å…±æŠ“å– {len(df)} ç¯‡æ–‡ç« ã€‚\")\n",
        "    return df, log_output\n",
        "\n",
        "\n",
        "# --- 3.2 è®€å¯« Google Sheet ---\n",
        "def get_or_create_worksheet(sheet, title):\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(title)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = sheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "    return worksheet\n",
        "\n",
        "def write_to_sheet(sheet, worksheet_name, df, log_output):\n",
        "    log_output.append(f\"--- 2. Google Sheet å¯«å…¥æ—¥èªŒ ---\")\n",
        "    try:\n",
        "        worksheet = get_or_create_worksheet(sheet, worksheet_name)\n",
        "        worksheet.clear()\n",
        "        worksheet.update(\n",
        "            [df.columns.values.tolist()] + df.astype(str).values.tolist(),\n",
        "            value_input_option=\"USER_ENTERED\"\n",
        "        )\n",
        "        log_output.append(f\"âœ… æˆåŠŸå¯«å…¥ {worksheet_name} å·¥ä½œè¡¨ ({len(df)} ç­†è³‡æ–™)ã€‚\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ å¯«å…¥ Sheet å¤±æ•—: {e}\")\n",
        "    return log_output\n",
        "\n",
        "# --- 3.3 TF-IDF é—œéµå­—åˆ†æ ---\n",
        "\n",
        "STOPWORDS = set(['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'ä¹‹', 'ä¸€å€‹', 'å’Œ',\n",
        "                 'è¨è«–', 'åˆ†äº«', 'å¿ƒå¾—', 'å•é¡Œ', 'è«‹ç›Š', 'æƒ…å ±', 'LIVE', 'Re', 're', 'EP', 'OST', 'EP', 'ep', '2025',\n",
        "                 'æ—¥åŠ‡', 'æ—¥', 'åŠ‡', 'é›»å½±', 'å½±']) # å¢åŠ èˆ‡çœ‹æ¿ç›¸é—œçš„è©\n",
        "\n",
        "def get_tfidf_keywords(df, top_n, log_output):\n",
        "    \"\"\"ä½¿ç”¨ sklearn.TfidfVectorizer é€²è¡Œ TF-IDF åˆ†æä¸¦è¨˜éŒ„è©³ç´°æ—¥èªŒ\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 3. TF-IDF åˆ†ææ—¥èªŒ (Sklearn) ---\")\n",
        "    log_output.append(f\"ç›®æ¨™é—œéµå­—æ•¸é‡: Top {top_n}\")\n",
        "    log_output.append(f\"åœç”¨è©æ•¸é‡: {len(STOPWORDS)}\")\n",
        "\n",
        "    if 'æ¨™é¡Œ' not in df.columns or df['æ¨™é¡Œ'].dropna().empty:\n",
        "        log_output.append(\"âŒ éŒ¯èª¤: è³‡æ–™é›†ä¸­ç¼ºå°‘ 'æ¨™é¡Œ' æ¬„ä½æˆ–æ¨™é¡Œç‚ºç©ºã€‚\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "    document_list = []\n",
        "\n",
        "    # 1. æ–‡æœ¬é è™•ç†èˆ‡åˆ†è©\n",
        "    for title in df['æ¨™é¡Œ'].dropna():\n",
        "        cleaned_text = re.sub(r\"\\[.*?\\]\", \"\", title).strip()\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', ' ', cleaned_text)\n",
        "        cleaned_text = re.sub(r'\\b(ep|re)\\d+\\b', ' ', cleaned_text, flags=re.IGNORECASE)\n",
        "\n",
        "        # ä½¿ç”¨ç²¾ç¢ºæ¨¡å¼åˆ†è©\n",
        "        words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "        # éæ¿¾åœç”¨è©å’Œå–®å­—\n",
        "        filtered_words = [\n",
        "            word.strip()\n",
        "            for word in words\n",
        "            if word.strip() and len(word.strip()) > 1 and word.strip().lower() not in STOPWORDS\n",
        "        ]\n",
        "\n",
        "        if filtered_words:\n",
        "            document_list.append(\" \".join(filtered_words))\n",
        "\n",
        "    log_output.append(f\"å·²å°‡ {len(df)} ç¯‡æ¨™é¡Œåˆ†è©ä¸¦éæ¿¾ï¼Œç”¢ç”Ÿ {len(document_list)} ç¯‡æœ‰æ•ˆæ–‡æª”ã€‚\")\n",
        "\n",
        "    if not document_list:\n",
        "        log_output.append(\"âš ï¸ æ²’æœ‰å¯åˆ†æçš„æ–‡æª” (å¯èƒ½éƒ½è¢«éæ¿¾äº†)ã€‚\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "    # 2. TF-IDF è¨ˆç®—\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "        log_output.append(f\"âœ… TF-IDF çŸ©é™£å»ºç«‹æˆåŠŸã€‚è©å½™ç¸½æ•¸ (Features): {len(feature_names)}\")\n",
        "\n",
        "        # 3. è¨ˆç®—å¹³å‡æ¬Šé‡ (æ‰¾å‡ºæ•´é«”é‡è¦æ€§)\n",
        "        avg_tfidf_scores = defaultdict(float)\n",
        "        num_documents = len(document_list)\n",
        "\n",
        "        for doc_weights in tfidf_array:\n",
        "            for i, weight in enumerate(doc_weights):\n",
        "                word = feature_names[i]\n",
        "                avg_tfidf_scores[word] += weight\n",
        "\n",
        "        for word in avg_tfidf_scores:\n",
        "            avg_tfidf_scores[word] /= num_documents\n",
        "\n",
        "        # 4. æ’åºä¸¦é¸å‡º Top N\n",
        "        sorted_avg_tfidf = sorted(avg_tfidf_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "        top_keywords_df = pd.DataFrame(sorted_avg_tfidf[:top_n], columns=['é—œéµå­—', 'TF-IDFå¹³å‡æ¬Šé‡'])\n",
        "\n",
        "        log_output.append(f\"âœ… æˆåŠŸæå– Top {len(top_keywords_df)} å€‹é—œéµå­—ã€‚\")\n",
        "        log_output.append(f\"Top 5 é—œéµå­—ç¯„ä¾‹: {', '.join([k[0] for k in sorted_avg_tfidf[:5]])}\")\n",
        "\n",
        "        return top_keywords_df, log_output\n",
        "\n",
        "    except ValueError as e:\n",
        "        log_output.append(f\"âŒ TF-IDF åˆ†æå¤±æ•— (ValueError): {e}\")\n",
        "        return pd.DataFrame(), log_output\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ TF-IDF åˆ†æç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "# --- 3.4 Gemini API ç”Ÿæˆæ‘˜è¦ (æ–°å¢æ—¥èªŒ) ---\n",
        "def get_gemini_summary(keywords_df, log_output):\n",
        "    \"\"\"ä½¿ç”¨ Gemini API æ ¹æ“šé—œéµå­—ç”Ÿæˆæ‘˜è¦\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 4. Gemini æ‘˜è¦æ—¥èªŒ ---\")\n",
        "    if keywords_df.empty:\n",
        "        log_output.append(\"âš ï¸ ç¼ºå°‘é—œéµå­—ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚\")\n",
        "        return \"âš ï¸ æ²’æœ‰é—œéµå­—ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚\", log_output\n",
        "\n",
        "    keywords_list = keywords_df['é—œéµå­—'].tolist()\n",
        "    prompt = f\"\"\"\n",
        "    æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­çš„æ•¸æ“šåˆ†æå¸«ï¼Œå°ˆç²¾æ–¼ç¤¾ç¾¤è¼¿æƒ…åˆ†æã€‚\n",
        "\n",
        "    ä»»å‹™ï¼š\n",
        "    è«‹æ ¹æ“š PTT æ—¥åŠ‡ç‰ˆ (JapanDrama) çš„ {len(keywords_list)} å€‹ç†±é–€é—œéµå­—ï¼Œç”Ÿæˆä¸€ä»½å°ˆæ¥­çš„åˆ†æå ±å‘Šã€‚\n",
        "\n",
        "    ç†±é–€é—œéµå­— (ä¾ TF-IDF å¹³å‡æ¬Šé‡æ’åº)ï¼š\n",
        "    {', '.join(keywords_list)}\n",
        "\n",
        "    è¼¸å‡ºæ ¼å¼è¦æ±‚ (è«‹åš´æ ¼éµå®ˆ)ï¼š\n",
        "    1.  **äº”å¥æ´å¯Ÿæ‘˜è¦**ï¼šæ¢åˆ—å¼ï¼Œæ¯å¥éƒ½æ˜¯ç²¾é—¢çš„è§€å¯Ÿã€‚\n",
        "    2.  **ä¸€æ®µ 120 å­—çµè«–**ï¼šç¸½çµç›®å‰çš„è¶¨å‹¢æˆ–ç¾è±¡ã€‚\n",
        "\n",
        "    è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        log_output.append(f\"æ¨¡å‹è«‹æ±‚åƒæ•¸: gemini-2.5-flash, é—œéµå­—æ•¸é‡: {len(keywords_list)}\")\n",
        "        response = model.generate_content(prompt, request_options={\"timeout\": 120})\n",
        "        clean_text = response.text.replace(\"#\", \"\").replace(\"*\", \"\")\n",
        "        log_output.append(\"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸã€‚\")\n",
        "        return clean_text, log_output\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"âŒ Gemini API å‘¼å«å¤±æ•—ï¼š{e}\")\n",
        "        return f\"âŒ Gemini API å‘¼å«å¤±æ•—ï¼š{e}\", log_output\n",
        "\n",
        "# --- 3.5 ç¨ç«‹çš„ Jieba åˆ†è©å±•ç¤ºå‡½å¼ (æ–°å¢) ---\n",
        "def demonstrate_jieba_modes(sentence):\n",
        "    \"\"\"å±•ç¤º Jieba çš„å››ç¨®åˆ†è©æ¨¡å¼\"\"\"\n",
        "    if not sentence:\n",
        "        return \"è«‹è¼¸å…¥ä¸­æ–‡å¥å­é€²è¡Œæ¸¬è©¦\", \"è«‹è¼¸å…¥ä¸­æ–‡å¥å­é€²è¡Œæ¸¬è©¦\", \"è«‹è¼¸å…¥ä¸­æ–‡å¥å­é€²è¡Œæ¸¬è©¦\", \"è«‹è¼¸å…¥ä¸­æ–‡å¥å­é€²è¡Œæ¸¬è©¦\"\n",
        "\n",
        "    output = f\"åŸå§‹å¥å­: {sentence}\\n\\n\"\n",
        "\n",
        "    # 1. ç²¾ç¢ºæ¨¡å¼ (Default Mode)\n",
        "    seg_list_precise = jieba.cut(sentence, cut_all=False)\n",
        "    precise_result = '/ '.join(seg_list_precise)\n",
        "\n",
        "    # 2. å…¨æ¨¡å¼ (Full Mode)\n",
        "    seg_list_all = jieba.cut(sentence, cut_all=True)\n",
        "    all_result = '/ '.join(seg_list_all)\n",
        "\n",
        "    # 3. æœå°‹å¼•æ“æ¨¡å¼ (Search Engine Mode)\n",
        "    seg_list_search = jieba.cut_for_search(sentence)\n",
        "    search_result = '/ '.join(seg_list_search)\n",
        "\n",
        "    # 4. è©æ€§æ¨™è¨»\n",
        "    words = pseg.cut(sentence)\n",
        "    pos_result_list = [f\"{word}/{flag}\" for word, flag in words]\n",
        "    pos_result = ' '.join(pos_result_list)\n",
        "\n",
        "    return precise_result, all_result, search_result, pos_result\n",
        "\n",
        "\n",
        "# --- 3.6 Gradio è¼”åŠ©å‡½å¼ (é»æ“Šè¡¨æ ¼) ---\n",
        "def show_selected_link(data: pd.DataFrame, evt: gr.SelectData):\n",
        "    \"\"\"ç•¶ä½¿ç”¨è€…é»æ“Š Dataframe ä¸­çš„æŸä¸€åˆ—æ™‚ï¼Œåœ¨ Markdown ä¸­é¡¯ç¤ºè©²åˆ—çš„é€£çµ\"\"\"\n",
        "    if evt.index is None or not evt.selected:\n",
        "        return \"*é»æ“Šä¸Šæ–¹è¡¨æ ¼ä¸­çš„ä»»ä¸€æ–‡ç« ï¼Œé€£çµå°‡é¡¯ç¤ºæ–¼æ­¤*\"\n",
        "    try:\n",
        "        selected_row_index = evt.index[0] # ç²å–åˆ—ç´¢å¼•\n",
        "        if selected_row_index >= len(data):\n",
        "             return \"*è¡¨æ ¼è³‡æ–™å·²è®Šå‹•ï¼Œè«‹é‡æ–°é»æ“Š*\"\n",
        "\n",
        "        selected_row = data.iloc[selected_row_index]\n",
        "        title = selected_row.get('æ¨™é¡Œ', 'ç„¡æ¨™é¡Œ')\n",
        "        link = selected_row.get('é€£çµ', None)\n",
        "\n",
        "        if link:\n",
        "            # Markdown é€£çµ (Gradio æœƒè‡ªå‹•ä½¿å…¶åœ¨æ–°åˆ†é é–‹å•Ÿ)\n",
        "            return f\"### é»æ“Šé–‹å•Ÿé€£çµï¼š\\n## [{title}]({link})\"\n",
        "        else:\n",
        "            return f\"**{title}**\\n\\n(æ­¤åˆ—æ²’æœ‰å¯ç”¨çš„ 'é€£çµ' æ¬„ä½)\"\n",
        "    except Exception as e:\n",
        "        return f\"ç„¡æ³•è®€å–é€£çµï¼š{e}\""
      ],
      "metadata": {
        "id": "hVGOC9SACHqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 4. Gradio æ•´åˆå‡½å¼\n",
        "# ================================\n",
        "def run_full_automation_flow(top_n_str, pages_to_fetch_str):\n",
        "    \"\"\"Gradio é»æ“Šå¾ŒåŸ·è¡Œçš„å®Œæ•´æµç¨‹\"\"\"\n",
        "\n",
        "    empty_df = pd.DataFrame()\n",
        "    empty_str = \"\"\n",
        "    log_output = []\n",
        "\n",
        "    site_list = [\"PTT æ—¥åŠ‡ç‰ˆ (Japandrama)\"]\n",
        "    # æ¸…ç©ºä¸Šæ¬¡çµæœ\n",
        "    yield \"æ—¥èªŒå°‡é¡¯ç¤ºæ–¼æ­¤...\", empty_df, empty_str, None, gr.Radio(choices=[\"å°šæœªåŸ·è¡Œ\"], value=\"å°šæœªåŸ·è¡Œ\"), empty_df\n",
        "\n",
        "    # --- åƒæ•¸é©—è­‰ ---\n",
        "    try:\n",
        "        top_n = int(top_n_str)\n",
        "        pages_to_fetch = int(pages_to_fetch_str)\n",
        "        if top_n <= 0 or pages_to_fetch <= 0:\n",
        "            log_output.append(\"âŒ Top N æˆ–çˆ¬å–é æ•¸å¿…é ˆæ˜¯å¤§æ–¼ 0 çš„æ•¸å­—ã€‚\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_df\n",
        "            return\n",
        "    except ValueError:\n",
        "        log_output.append(\"âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ã€‚\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_df\n",
        "        return\n",
        "\n",
        "    # --- è‡ªå‹•åŒ–æµç¨‹ ---\n",
        "    log_output.append(\"===================================================\")\n",
        "    log_output.append(f\"ğŸš€ è‡ªå‹•åŒ–æµç¨‹å•Ÿå‹• ({datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\")\n",
        "    log_output.append(\"===================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- æ­¥é©Ÿ 1: çˆ¬èŸ² ---\n",
        "        log_output.append(\"1/4: ğŸƒâ€â™‚ï¸ é–‹å§‹çˆ¬å– PTT JapanDrama ç‰ˆæ–‡ç« ...\")\n",
        "        # [ä¿®æ­£ Yield 1] è£œä¸Šç¬¬ 5 å€‹å€¼ empty_df\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), empty_df\n",
        "\n",
        "        scraped_df, log_output = scrape_ptt_japandrama(pages_to_fetch, log_output)\n",
        "        # [ä¿®æ­£ Yield 2] scraped_df å·²æ›´æ–°\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        if scraped_df.empty:\n",
        "            log_output.append(\"âŒ çˆ¬èŸ²å¤±æ•—ï¼ŒæœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ã€‚æµç¨‹çµ‚æ­¢ã€‚\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_df\n",
        "            return\n",
        "\n",
        "        # --- æ­¥é©Ÿ 2: å¯«å…¥ Sheet (å¯é¸) ---\n",
        "        log_output = write_to_sheet(gsheets, \"PTTæ–‡ç« åˆ—è¡¨\", scraped_df, log_output)\n",
        "\n",
        "        # --- æ­¥é©Ÿ 3: TF-IDF åˆ†æ ---\n",
        "        log_output.append(\"2/4: ğŸ“Š æ­£åœ¨é€²è¡Œ Sklearn TF-IDF é—œéµå­—åˆ†æ...\")\n",
        "        # [ä¿®æ­£ Yield 3] è£œä¸Šç¬¬ 5 å€‹å€¼ scraped_df (å› ç‚ºçˆ¬èŸ²åˆ—è¡¨æ‡‰ç¹¼çºŒé¡¯ç¤º)\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        keywords_df, log_output = get_tfidf_keywords(scraped_df, top_n, log_output)\n",
        "\n",
        "        plot_df = None # å…ˆå®£å‘Š\n",
        "        if not keywords_df.empty:\n",
        "          # ç‚ºäº†è®“ BarPlot é ‚éƒ¨é¡¯ç¤ºæ¬Šé‡æœ€é«˜çš„ï¼Œæˆ‘å€‘éœ€å°‡ df å‡å†ªæ’åº\n",
        "          plot_df = keywords_df.sort_values(\"TF-IDFå¹³å‡æ¬Šé‡\", ascending=True)\n",
        "\n",
        "        # [ä¿®æ­£ Yield 4] ç¬¬ 2 å€‹å€¼æ›´æ–°ç‚º keywords_df\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        if keywords_df.empty:\n",
        "            log_output.append(\"âš ï¸ åˆ†æå®Œæˆï¼Œä½†æœªæå–åˆ°é—œéµå­—ã€‚æµç¨‹çµ‚æ­¢ã€‚\")\n",
        "            # [ä¿®æ­£ Yield 5] ç¬¬ 2 å€‹å€¼æ˜¯ empty_df (æ­£ç¢º)ï¼Œä½†ç¬¬ 5 å€‹å€¼ scraped_df æ‡‰ä¿ç•™\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), scraped_df\n",
        "            return\n",
        "\n",
        "        # --- æ­¥é©Ÿ 4: å¯«å…¥ Sheet (å¯é¸) ---\n",
        "        log_output.append(\"3/4: ğŸ“ˆ æ­£åœ¨å°‡ Top ç†±è©å›å¯«è‡³ Sheet (ç†±è©çµ±è¨ˆ)...\")\n",
        "        log_output = write_to_sheet(gsheets, \"ç†±è©çµ±è¨ˆ\", keywords_df, log_output)\n",
        "\n",
        "        # --- æ­¥é©Ÿ 5: Gemini æ‘˜è¦ ---\n",
        "        log_output.append(\"4/4: ğŸ§  æ­£åœ¨å‘¼å« Gemini API ç”Ÿæˆæ‘˜è¦...\")\n",
        "        # [ä¿®æ­£ Yield 6] ç¬¬ 2 å€‹å€¼æ˜¯ keywords_df (æ‡‰ä¿ç•™)ï¼Œè£œä¸Šç¬¬ 5 å€‹å€¼ scraped_df\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        summary, log_output = get_gemini_summary(keywords_df, log_output)\n",
        "      \t# [ä¿®æ­£ Yield 7] ç¬¬ 2 å€‹å€¼æ˜¯ keywords_dfï¼Œç¬¬ 3 å€‹å€¼æ›´æ–°ç‚º summary\n",
        "        yield \"\\n\".join(log_output), keywords_df, summary, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        log_output.append(\"===================================================\")\n",
        "        log_output.append(\"âœ… å…¨éƒ¨æµç¨‹å®Œæˆï¼è«‹åˆ‡æ›åˆ°ã€Œæœ€çµ‚çµæœã€æ¨™ç±¤é æŸ¥çœ‹ã€‚\")\n",
        "\n",
        "        # æœ€çµ‚å›å‚³ (æ‚¨åŸæœ¬é€™è¡Œå°±æ˜¯å°çš„)\n",
        "        yield \"\\n\".join(log_output), keywords_df, summary, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"âŒ æµç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼š{e}\\n{traceback.format_exc()}\"\n",
        "        log_output.append(error_msg)\n",
        "      \t# [ä¿®æ­£ Yield 8] ç¢ºä¿éŒ¯èª¤å›å‚³ä¹Ÿæ˜¯ 5 å€‹å€¼\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), empty_df"
      ],
      "metadata": {
        "id": "XNcmZqtMBHVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5. å•Ÿå‹• Gradio ä»‹é¢ (å·²å‡ç´š Tab ä½ˆå±€)\n",
        "# ================================\n",
        "print(\"\\nğŸš€ æ­£åœ¨å•Ÿå‹• Gradio ä»‹é¢...\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # ğŸ“Š PTT ç†±è©åˆ†æèˆ‡æ‘˜è¦ç³»çµ±\n",
        "\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"ğŸš€ è‡ªå‹•åŒ–æµç¨‹åŸ·è¡Œ\"):\n",
        "        with gr.Row():\n",
        "            pages_to_fetch_input = gr.Textbox(label=\"è¦çˆ¬å–çš„é æ•¸\", value=\"10\", scale=1)\n",
        "            top_n_input = gr.Textbox(label=\"è¦çµ±è¨ˆçš„ Top N ç†±è©æ•¸é‡\", value=\"20\", scale=1)\n",
        "            run_btn = gr.Button(\"ğŸš€ ä¸€éµå•Ÿå‹•è‡ªå‹•åŒ–æµç¨‹\", variant=\"primary\", scale=2)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # ä½¿ç”¨ Tab ä¾†å€åˆ†æœ€çµ‚çµæœå’Œæ—¥èªŒ\n",
        "        with gr.Tabs():\n",
        "\n",
        "            with gr.TabItem(\"ğŸ› ï¸ æŠ€è¡“æ—¥èªŒèˆ‡è¼¸å‡ºç´°ç¯€\"):\n",
        "                # ä½¿ç”¨ Textbox ä¾†é¡¯ç¤ºè©³ç´°çš„æ—¥èªŒ\n",
        "                log_output_text = gr.Textbox(\n",
        "                    label=\"è©³ç´°æµç¨‹æ—¥èªŒ (çˆ¬èŸ²ã€å¯«å…¥ã€åˆ†ææ­¥é©Ÿ)\",\n",
        "                    lines=30,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "            with gr.TabItem(\"ğŸ•¸ï¸ çˆ¬å–ä¹‹ç¶²ç«™\"):\n",
        "                site_list_output = gr.Radio(\n",
        "                    label=\"è³‡æ–™ä¾†æº\",\n",
        "                    choices=[\"å°šæœªåŸ·è¡Œ\"],\n",
        "                    value=\"å°šæœªåŸ·è¡Œ\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                gr.Markdown(\"---\")\n",
        "\n",
        "                link_display_output = gr.Markdown(\n",
        "          \t\t\tvalue=\"*é»æ“Šä¸Šæ–¹è¡¨æ ¼ä¸­çš„ä»»ä¸€æ–‡ç« ï¼Œé€£çµå°‡é¡¯ç¤ºæ–¼æ­¤*\"\n",
        "          \t\t  )\n",
        "\n",
        "                scraped_data_output = gr.Dataframe(\n",
        "              \t\tlabel=\"çˆ¬å–æ–‡ç« åˆ—è¡¨ (åŸå§‹è³‡æ–™)\",\n",
        "              \t\theaders=[\"æ—¥æœŸ\", \"ä½œè€…\", \"æ¨™é¡Œ\", \"é€£çµ\"], # é å…ˆå®šç¾©æ¬„ä½\n",
        "              \t\tinteractive=True,\n",
        "              \t\trow_count=(15, 'dynamic')\n",
        "              \t)\n",
        "\n",
        "\n",
        "\n",
        "            with gr.TabItem(\"âœ… æœ€çµ‚çµæœ\"):\n",
        "                summary_output = gr.Markdown(label=\"ğŸ¤– Gemini æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–\")\n",
        "\n",
        "                keyword_plot_output = gr.BarPlot(\n",
        "                  label=\"ğŸ“ˆ Top N ç†±è©è¦–è¦ºåŒ–åœ–è¡¨\",\n",
        "                  x=\"TF-IDFå¹³å‡æ¬Šé‡\", # X è»¸ (å€¼)\n",
        "                  y=\"é—œéµå­—\",       # Y è»¸ (é¡åˆ¥)\n",
        "                  tooltip=['é—œéµå­—', 'TF-IDFå¹³å‡æ¬Šé‡'],\n",
        "                  color=\"TF-IDFå¹³å‡æ¬Šé‡\",\n",
        "                  vertical=False, # è¨­ç‚ºæ°´å¹³é•·æ¢åœ–\n",
        "                  height=400\n",
        "                )\n",
        "\n",
        "\n",
        "                keywords_output = gr.Dataframe(label=\"ğŸ“ˆ Top N ç†±è©çµ±è¨ˆçµæœ (Sklearn TF-IDF å¹³å‡æ¬Šé‡)\")\n",
        "\n",
        "\n",
        "        # è¨­å®šé»æ“Šäº‹ä»¶\n",
        "        run_btn.click(\n",
        "          fn=run_full_automation_flow,\n",
        "          \tinputs=[top_n_input, pages_to_fetch_input],\n",
        "          \toutputs=[\n",
        "                log_output_text,\n",
        "                keywords_output,\n",
        "                summary_output,      # 3. æ‘˜è¦ (Markdown)\n",
        "      \t\t      keyword_plot_output,\n",
        "                site_list_output,\n",
        "                scraped_data_output\n",
        "          \t]\n",
        "        )\n",
        "\n",
        "        scraped_data_output.select(\n",
        "      \t\tfn=show_selected_link,\n",
        "      \t\tinputs=[scraped_data_output], # å°‡ Dataframe æœ¬èº«ä½œç‚ºè¼¸å…¥\n",
        "      \t\toutputs=[link_display_output], # å°‡é€£çµè¼¸å‡ºåˆ° Markdown\n",
        "      \t\tshow_progress=False # é»æ“Šæ™‚ä¸éœ€è¦é¡¯ç¤º \"loading\"\n",
        "    \t  )\n",
        "\n",
        "    with gr.Tab(\"ğŸ”¬ Jieba åˆ†è©æ¨¡å¼æ¸¬è©¦\"):\n",
        "        gr.Markdown(\"### ä¸­æ–‡åˆ†è©æ¨¡å¼æ¯”è¼ƒ\")\n",
        "        jieba_input = gr.Textbox(\n",
        "            label=\"è¼¸å…¥ä¸­æ–‡å¥å­ (ä¾‹å¦‚: æ—¥æœ¬é›»è¦–å°èˆ‡è£½ä½œå…¬å¸åˆä½œæ¨å‡ºé©šæ‚šæ–°åŠ‡)\",\n",
        "            value=\"æ—¥æœ¬é›»è¦–å°èˆ‡è£½ä½œå…¬å¸åˆä½œæ¨å‡ºé©šæ‚šæ–°åŠ‡\",\n",
        "            lines=2\n",
        "        )\n",
        "\n",
        "        jieba_run_btn = gr.Button(\"ğŸ” åŸ·è¡Œåˆ†æ\")\n",
        "\n",
        "        # ä½¿ç”¨ Row ä¾†æ°´å¹³å±•ç¤ºå››ç¨®æ¨¡å¼\n",
        "        with gr.Row():\n",
        "            precise_output = gr.Textbox(label=\"1. ç²¾ç¢ºæ¨¡å¼ (æœ€å¸¸ç”¨)\")\n",
        "            all_output = gr.Textbox(label=\"2. å…¨æ¨¡å¼ (æ‰€æœ‰å¯èƒ½çµ„åˆ)\")\n",
        "        with gr.Row():\n",
        "            search_output = gr.Textbox(label=\"3. æœå°‹å¼•æ“æ¨¡å¼ (é•·è©å†åˆ‡)\")\n",
        "            pos_output = gr.Textbox(label=\"4. è©æ€§æ¨™è¨» (è©èª/è©æ€§)\")\n",
        "\n",
        "        jieba_run_btn.click(\n",
        "            fn=demonstrate_jieba_modes,\n",
        "            inputs=[jieba_input],\n",
        "            outputs=[precise_output, all_output, search_output, pos_output],\n",
        "        )\n",
        "\n",
        "\n",
        "demo.queue().launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "kd6_uccnCdmD",
        "outputId": "8f231987-77c7-40d5-aeed-24312fce9c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ æ­£åœ¨å•Ÿå‹• Gradio ä»‹é¢...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://db54f8bc2c74d9575d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://db54f8bc2c74d9575d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://db54f8bc2c74d9575d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}