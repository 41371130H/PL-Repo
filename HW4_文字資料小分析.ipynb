{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa/LPkHLdUYStt0s/EchZT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41371130H/PL-Repo/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 PTT 熱詞分析與摘要系統 (Gradio 介面)\n",
        "\n",
        "*歡迎使用本系統！這是一個專為分析 PTT 日劇版 (Japandrama) 輿情而設計的自動化工具。*\n",
        "\n",
        "*您可以透過這個網頁介面，一鍵啟動爬蟲抓取最新文章、分析熱門關鍵字，並利用 Gemini 自動生成專業的趨勢摘要。*\n",
        "\n",
        "Google Sheet 連結：https://docs.google.com/spreadsheets/d/1mw8T-_jsLFGHZcq1EpreAZFjdY4TXoH-uPOtGyBKQW4/edit?usp=sharing\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 自動化流程執行 (主要功能)\n",
        "\n",
        "這是本系統的核心功能頁籤。\n",
        "\n",
        "### **1. 參數輸入**\n",
        "\n",
        "在啟動流程前，您需要設定兩個參數：\n",
        "\n",
        "* **要爬取的頁數**：設定要從 PTT 日劇版抓取多少頁的文章列表 (例如輸入 10，代表抓取最新的 10 頁)。\n",
        "* **要統計的 Top N 熱詞數量**：設定您希望系統分析出前幾名的熱門關鍵字 (例如輸入 20，代表找出 Top 20 熱詞)。\n",
        "* **🚀 一鍵啟動自動化流程**：設定好參數後，點擊此按鈕即可開始執行。系統會自動完成所有分析步驟。\n",
        "\n",
        "### **2. 分析結果 (子分頁)**\n",
        "\n",
        "分析開始後，您可以在下方的三個子分頁中查看即時進度與最終結果：\n",
        "\n",
        "#### **🛠️ 技術日誌與輸出細節**\n",
        "*這個分頁用來追蹤系統目前的執行狀態。*\n",
        "* **詳細流程日誌**：系統會即時更新目前的進度，如果流程不幸發生錯誤，詳細的錯誤訊息也會顯示在此處。\n",
        "\n",
        "#### **🕸️ 爬取之網站**\n",
        "*這個分頁用來檢視原始資料。*\n",
        "* **資料來源**：顯示目前爬取的看板 (例如：PTT 日劇版)。\n",
        "* **網站連結**：點擊表格中的文章，可在上方顯示標題與連結，並於新分頁開啟原始 PTT 文章。\n",
        "* **爬取文章列表 (原始資料)**：以表格形式顯示系統從 PTT 抓取到的所有文章標題、作者、日期與連結。\n",
        "\n",
        "#### **✅ 最終結果**\n",
        "*這是您最需要關注的分頁。* 這裡會顯示分析的最終產出：\n",
        "* **🤖 Gemini 洞察摘要與結論**：由 AI 根據熱詞生成的專業分析報告，包含趨勢洞察與結論。\n",
        "* **📈 Top N 熱詞視覺化圖表**：將前 N 名的熱門關鍵字以水平長條圖顯示，讓您一眼看出哪些詞彙最熱門。\n",
        "* **📈 Top N 熱詞統計結果 (Data)**：顯示圖表的原始數據，包含關鍵字及其 TF-IDF 權重分數。\n",
        "\n",
        "---\n",
        "\n",
        "## 🔬 Jieba 分詞模式測試 (輔助工具)\n",
        "\n",
        "這是一個獨立的小工具，用來測試中文分詞的效果。\n",
        "\n",
        "1.  **輸入中文句子**：在輸入框中貼上您想測試的任何中文句子。\n",
        "2.  **🔍 執行分析**：點擊按鈕。\n",
        "3.  **查看結果**：下方會立即顯示四種不同的分詞模式結果，幫助您了解不同演算法是如何切分詞彙的。\n",
        "\n",
        "---\n",
        "\n",
        "## ☁️ 雲端自動備份 (後台功能)\n",
        "\n",
        "除了在介面上顯示結果外，當您按下「一鍵啟動」時，系統也會在背景執行：\n",
        "\n",
        "1.  將「爬取文章列表 (原始資料)」完整備份到指定的 **Google Sheets** 工作表 (\"PTT文章列表\")。\n",
        "2.  將「Top N 熱詞統計結果」備份到另一張 **Google Sheets** 工作表 (\"熱詞統計\")。\n"
      ],
      "metadata": {
        "id": "HejDpicwru0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-UNeO8r92_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ce7cdc-5323-4163-f5af-10d640c984fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.26.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.185.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gspread google-generativeai gradio requests beautifulsoup4 jieba pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Google 授權\n",
        "# ================================\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# ================================\n",
        "# 2. 檢查 Gemini API 金鑰\n",
        "# ================================\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    api_key = userdata.get(\"gemini\")\n",
        "except Exception as e:\n",
        "    print(f\"🚨 讀取金鑰時發生錯誤：{e}\")"
      ],
      "metadata": {
        "id": "uvd8o1S3A_m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import pytz\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import jieba  # 使用針對繁體中文優化的 jieba\n",
        "import jieba.analyse\n",
        "import jieba.posseg as pseg\n",
        "import google.generativeai as genai\n",
        "from google.colab import auth, userdata\n",
        "from google.auth import default\n",
        "import time\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import traceback"
      ],
      "metadata": {
        "id": "EYSAREYqBQJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "有改模型"
      ],
      "metadata": {
        "id": "EdbrKbRXB6xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Sheets 與 Gemini 授權與設定\n",
        "try:\n",
        "    # --- Google Sheets ---\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1mw8T-_jsLFGHZcq1EpreAZFjdY4TXoH-uPOtGyBKQW4/edit?usp=sharing\"\n",
        "    gsheets = gc.open_by_url(SPREADSHEET_URL)\n",
        "\n",
        "    # --- Gemini API ---\n",
        "    GEMINI_API_KEY = userdata.get(\"gemini\")\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"🚨 授權或設定時發生錯誤：{e}\")"
      ],
      "metadata": {
        "id": "7K1Oc3ByBdqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.1 PTT 爬蟲 (Helper 函式) ---\n",
        "def get_previous_page_url(soup):\n",
        "    \"\"\"獲取上一頁 (更舊的文章) 的連結\"\"\"\n",
        "    paging_div = soup.find('div', class_='btn-group btn-group-paging')\n",
        "    if paging_div:\n",
        "        prev_button = paging_div.find_all('a')[1]\n",
        "        if 'href' in prev_button.attrs:\n",
        "            return \"https://www.ptt.cc\" + prev_button['href']\n",
        "    return None\n",
        "\n",
        "def extract_index_from_url(url):\n",
        "    \"\"\"從 PTT 網址中解析出頁碼\"\"\"\n",
        "    try:\n",
        "        # 使用正規表達式匹配 index 後面的數字\n",
        "        match = re.search(r'index(\\d+)\\.html', url)\n",
        "        if match:\n",
        "             return int(match.group(1))\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# --- 3.1 PTT 爬蟲 (主要函式) ---\n",
        "def scrape_ptt_japandrama(pages_to_fetch, log_output):\n",
        "    \"\"\"爬取 PTT Japandrama 版指定頁數的文章列表，並記錄詳細日誌\"\"\"\n",
        "    BOARD_NAME = \"Japandrama\"\n",
        "    START_URL = f\"https://www.ptt.cc/bbs/{BOARD_NAME}/index.html\"\n",
        "    BASE_URL = f\"https://www.ptt.cc/bbs/{BOARD_NAME}/index\"\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.post(\"https://www.ptt.cc/ask/over18\", data={\"yes\": \"yes\"})\n",
        "\n",
        "    all_data_list = []\n",
        "    log_output.append(f\"--- 1. 爬蟲日誌 ---\")\n",
        "    log_output.append(f\"目標看板: {BOARD_NAME} | 爬取頁數: {pages_to_fetch}\")\n",
        "\n",
        "    try:\n",
        "        # 1. 取得最新的頁碼\n",
        "        r = session.get(START_URL, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        prev_page_url = get_previous_page_url(soup)\n",
        "\n",
        "        if not prev_page_url:\n",
        "            log_output.append(\"❌ 錯誤：找不到 '上一頁' 連結，無法定位最新頁碼。\")\n",
        "            return pd.DataFrame(), log_output\n",
        "\n",
        "        start_index = extract_index_from_url(prev_page_url)\n",
        "        if start_index is None:\n",
        "            log_output.append(\"❌ 錯誤：無法從連結中解析出起始頁碼。\")\n",
        "            return pd.DataFrame(), log_output\n",
        "\n",
        "        log_output.append(f\"起始頁碼 (前一頁): {start_index}。將從 {start_index} 遞減爬取。\")\n",
        "\n",
        "        # 先爬取最新一頁\n",
        "        log_output.append(f\" -> 爬取頁面 (最新): {START_URL}\")\n",
        "        articles = soup.find_all(\"div\", class_=\"r-ent\")\n",
        "        for art in articles:\n",
        "          title_tag = art.find(\"div\", class_=\"title\").find(\"a\")\n",
        "          if title_tag and \"[公告]\" not in title_tag.text:\n",
        "            title = title_tag.text\n",
        "            link = \"https://www.ptt.cc\" + title_tag[\"href\"]\n",
        "            author = art.find(\"div\", class_=\"author\").text\n",
        "            date_str = art.find(\"div\", class_='date').text.strip()\n",
        "            all_data_list.append({\n",
        "              \"日期\": date_str, \"作者\": author, \"標題\": title, \"連結\": link\n",
        "            })\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "        # 2. 迴圈爬取\n",
        "        pages_remaining = pages_to_fetch - 1\n",
        "        if pages_remaining > 0:\n",
        "          stop_index = start_index - pages_remaining\n",
        "          for index in range(start_index, stop_index, -1):\n",
        "              url = f\"{BASE_URL}{index}.html\"\n",
        "              log_output.append(f\" -> 爬取頁面 {index}:{url}\")\n",
        "\n",
        "              try:\n",
        "                  r_page = session.get(url, timeout=10)\n",
        "                  if r_page.status_code != 200:\n",
        "                      log_output.append(f\"   ⚠️ 跳過頁面 (狀態碼: {r_page.status_code})\")\n",
        "                      continue\n",
        "\n",
        "                  soup_page = BeautifulSoup(r_page.text, \"html.parser\")\n",
        "                  articles = soup_page.find_all(\"div\", class_=\"r-ent\")\n",
        "\n",
        "                  for art in articles:\n",
        "                      title_tag = art.find(\"div\", class_=\"title\").find(\"a\")\n",
        "                      if title_tag and \"[公告]\" not in title_tag.text:\n",
        "                          title = title_tag.text\n",
        "                          link = \"https://www.ptt.cc\" + title_tag[\"href\"]\n",
        "                          author = art.find(\"div\", class_=\"author\").text\n",
        "                          date_str = art.find(\"div\", class_='date').text.strip()\n",
        "                          all_data_list.append({\n",
        "                              \"日期\": date_str, \"作者\": author, \"標題\": title, \"連結\": link\n",
        "                          })\n",
        "\n",
        "                  time.sleep(0.1) # 縮短延遲以加速示範\n",
        "\n",
        "              except requests.exceptions.RequestException as e:\n",
        "                  log_output.append(f\"   ❌ 爬取失敗: {e}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_output.append(f\"❌ 爬蟲起始請求失敗：{e}\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "    df = pd.DataFrame(all_data_list)\n",
        "    log_output.append(f\"✅ 爬蟲結束。共抓取 {len(df)} 篇文章。\")\n",
        "    return df, log_output\n",
        "\n",
        "\n",
        "# --- 3.2 讀寫 Google Sheet ---\n",
        "def get_or_create_worksheet(sheet, title):\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(title)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = sheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "    return worksheet\n",
        "\n",
        "def write_to_sheet(sheet, worksheet_name, df, log_output):\n",
        "    log_output.append(f\"--- 2. Google Sheet 寫入日誌 ---\")\n",
        "    try:\n",
        "        worksheet = get_or_create_worksheet(sheet, worksheet_name)\n",
        "        worksheet.clear()\n",
        "        worksheet.update(\n",
        "            [df.columns.values.tolist()] + df.astype(str).values.tolist(),\n",
        "            value_input_option=\"USER_ENTERED\"\n",
        "        )\n",
        "        log_output.append(f\"✅ 成功寫入 {worksheet_name} 工作表 ({len(df)} 筆資料)。\")\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ 寫入 Sheet 失敗: {e}\")\n",
        "    return log_output\n",
        "\n",
        "# --- 3.3 TF-IDF 關鍵字分析 ---\n",
        "\n",
        "STOPWORDS = set(['的', '了', '是', '在', '我', '你', '他', '她', '之', '一個', '和',\n",
        "                 '討論', '分享', '心得', '問題', '請益', '情報', 'LIVE', 'Re', 're', 'EP', 'OST', 'EP', 'ep', '2025',\n",
        "                 '日劇', '日', '劇', '電影', '影']) # 增加與看板相關的詞\n",
        "\n",
        "def get_tfidf_keywords(df, top_n, log_output):\n",
        "    \"\"\"使用 sklearn.TfidfVectorizer 進行 TF-IDF 分析並記錄詳細日誌\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 3. TF-IDF 分析日誌 (Sklearn) ---\")\n",
        "    log_output.append(f\"目標關鍵字數量: Top {top_n}\")\n",
        "    log_output.append(f\"停用詞數量: {len(STOPWORDS)}\")\n",
        "\n",
        "    if '標題' not in df.columns or df['標題'].dropna().empty:\n",
        "        log_output.append(\"❌ 錯誤: 資料集中缺少 '標題' 欄位或標題為空。\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "    document_list = []\n",
        "\n",
        "    # 1. 文本預處理與分詞\n",
        "    for title in df['標題'].dropna():\n",
        "        cleaned_text = re.sub(r\"\\[.*?\\]\", \"\", title).strip()\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', ' ', cleaned_text)\n",
        "        cleaned_text = re.sub(r'\\b(ep|re)\\d+\\b', ' ', cleaned_text, flags=re.IGNORECASE)\n",
        "\n",
        "        # 使用精確模式分詞\n",
        "        words = jieba.lcut(cleaned_text, cut_all=False)\n",
        "\n",
        "        # 過濾停用詞和單字\n",
        "        filtered_words = [\n",
        "            word.strip()\n",
        "            for word in words\n",
        "            if word.strip() and len(word.strip()) > 1 and word.strip().lower() not in STOPWORDS\n",
        "        ]\n",
        "\n",
        "        if filtered_words:\n",
        "            document_list.append(\" \".join(filtered_words))\n",
        "\n",
        "    log_output.append(f\"已將 {len(df)} 篇標題分詞並過濾，產生 {len(document_list)} 篇有效文檔。\")\n",
        "\n",
        "    if not document_list:\n",
        "        log_output.append(\"⚠️ 沒有可分析的文檔 (可能都被過濾了)。\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "    # 2. TF-IDF 計算\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "        log_output.append(f\"✅ TF-IDF 矩陣建立成功。詞彙總數 (Features): {len(feature_names)}\")\n",
        "\n",
        "        # 3. 計算平均權重 (找出整體重要性)\n",
        "        avg_tfidf_scores = defaultdict(float)\n",
        "        num_documents = len(document_list)\n",
        "\n",
        "        for doc_weights in tfidf_array:\n",
        "            for i, weight in enumerate(doc_weights):\n",
        "                word = feature_names[i]\n",
        "                avg_tfidf_scores[word] += weight\n",
        "\n",
        "        for word in avg_tfidf_scores:\n",
        "            avg_tfidf_scores[word] /= num_documents\n",
        "\n",
        "        # 4. 排序並選出 Top N\n",
        "        sorted_avg_tfidf = sorted(avg_tfidf_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "        top_keywords_df = pd.DataFrame(sorted_avg_tfidf[:top_n], columns=['關鍵字', 'TF-IDF平均權重'])\n",
        "\n",
        "        log_output.append(f\"✅ 成功提取 Top {len(top_keywords_df)} 個關鍵字。\")\n",
        "        log_output.append(f\"Top 5 關鍵字範例: {', '.join([k[0] for k in sorted_avg_tfidf[:5]])}\")\n",
        "\n",
        "        return top_keywords_df, log_output\n",
        "\n",
        "    except ValueError as e:\n",
        "        log_output.append(f\"❌ TF-IDF 分析失敗 (ValueError): {e}\")\n",
        "        return pd.DataFrame(), log_output\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ TF-IDF 分析發生未預期錯誤: {e}\")\n",
        "        return pd.DataFrame(), log_output\n",
        "\n",
        "# --- 3.4 Gemini API 生成摘要 (新增日誌) ---\n",
        "def get_gemini_summary(keywords_df, log_output):\n",
        "    \"\"\"使用 Gemini API 根據關鍵字生成摘要\"\"\"\n",
        "\n",
        "    log_output.append(f\"--- 4. Gemini 摘要日誌 ---\")\n",
        "    if keywords_df.empty:\n",
        "        log_output.append(\"⚠️ 缺少關鍵字，無法生成摘要。\")\n",
        "        return \"⚠️ 沒有關鍵字，無法生成摘要。\", log_output\n",
        "\n",
        "    keywords_list = keywords_df['關鍵字'].tolist()\n",
        "    prompt = f\"\"\"\n",
        "    您是一位專業的數據分析師，專精於社群輿情分析。\n",
        "\n",
        "    任務：\n",
        "    請根據 PTT 日劇版 (JapanDrama) 的 {len(keywords_list)} 個熱門關鍵字，生成一份專業的分析報告。\n",
        "\n",
        "    熱門關鍵字 (依 TF-IDF 平均權重排序)：\n",
        "    {', '.join(keywords_list)}\n",
        "\n",
        "    輸出格式要求 (請嚴格遵守)：\n",
        "    1.  **五句洞察摘要**：條列式，每句都是精闢的觀察。\n",
        "    2.  **一段 120 字結論**：總結目前的趨勢或現象。\n",
        "\n",
        "    請使用繁體中文回答。\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        log_output.append(f\"模型請求參數: gemini-2.5-flash, 關鍵字數量: {len(keywords_list)}\")\n",
        "        response = model.generate_content(prompt, request_options={\"timeout\": 120})\n",
        "        clean_text = response.text.replace(\"#\", \"\").replace(\"*\", \"\")\n",
        "        log_output.append(\"✅ 摘要生成成功。\")\n",
        "        return clean_text, log_output\n",
        "    except Exception as e:\n",
        "        log_output.append(f\"❌ Gemini API 呼叫失敗：{e}\")\n",
        "        return f\"❌ Gemini API 呼叫失敗：{e}\", log_output\n",
        "\n",
        "# --- 3.5 獨立的 Jieba 分詞展示函式 (新增) ---\n",
        "def demonstrate_jieba_modes(sentence):\n",
        "    \"\"\"展示 Jieba 的四種分詞模式\"\"\"\n",
        "    if not sentence:\n",
        "        return \"請輸入中文句子進行測試\", \"請輸入中文句子進行測試\", \"請輸入中文句子進行測試\", \"請輸入中文句子進行測試\"\n",
        "\n",
        "    output = f\"原始句子: {sentence}\\n\\n\"\n",
        "\n",
        "    # 1. 精確模式 (Default Mode)\n",
        "    seg_list_precise = jieba.cut(sentence, cut_all=False)\n",
        "    precise_result = '/ '.join(seg_list_precise)\n",
        "\n",
        "    # 2. 全模式 (Full Mode)\n",
        "    seg_list_all = jieba.cut(sentence, cut_all=True)\n",
        "    all_result = '/ '.join(seg_list_all)\n",
        "\n",
        "    # 3. 搜尋引擎模式 (Search Engine Mode)\n",
        "    seg_list_search = jieba.cut_for_search(sentence)\n",
        "    search_result = '/ '.join(seg_list_search)\n",
        "\n",
        "    # 4. 詞性標註\n",
        "    words = pseg.cut(sentence)\n",
        "    pos_result_list = [f\"{word}/{flag}\" for word, flag in words]\n",
        "    pos_result = ' '.join(pos_result_list)\n",
        "\n",
        "    return precise_result, all_result, search_result, pos_result\n",
        "\n",
        "\n",
        "# --- 3.6 Gradio 輔助函式 (點擊表格) ---\n",
        "def show_selected_link(data: pd.DataFrame, evt: gr.SelectData):\n",
        "    \"\"\"當使用者點擊 Dataframe 中的某一列時，在 Markdown 中顯示該列的連結\"\"\"\n",
        "    if evt.index is None or not evt.selected:\n",
        "        return \"*點擊上方表格中的任一文章，連結將顯示於此*\"\n",
        "    try:\n",
        "        selected_row_index = evt.index[0] # 獲取列索引\n",
        "        if selected_row_index >= len(data):\n",
        "             return \"*表格資料已變動，請重新點擊*\"\n",
        "\n",
        "        selected_row = data.iloc[selected_row_index]\n",
        "        title = selected_row.get('標題', '無標題')\n",
        "        link = selected_row.get('連結', None)\n",
        "\n",
        "        if link:\n",
        "            # Markdown 連結 (Gradio 會自動使其在新分頁開啟)\n",
        "            return f\"### 點擊開啟連結：\\n## [{title}]({link})\"\n",
        "        else:\n",
        "            return f\"**{title}**\\n\\n(此列沒有可用的 '連結' 欄位)\"\n",
        "    except Exception as e:\n",
        "        return f\"無法讀取連結：{e}\""
      ],
      "metadata": {
        "id": "hVGOC9SACHqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 4. Gradio 整合函式\n",
        "# ================================\n",
        "def run_full_automation_flow(top_n_str, pages_to_fetch_str):\n",
        "    \"\"\"Gradio 點擊後執行的完整流程\"\"\"\n",
        "\n",
        "    empty_df = pd.DataFrame()\n",
        "    empty_str = \"\"\n",
        "    log_output = []\n",
        "\n",
        "    site_list = [\"PTT 日劇版 (Japandrama)\"]\n",
        "    # 清空上次結果\n",
        "    yield \"日誌將顯示於此...\", empty_df, empty_str, None, gr.Radio(choices=[\"尚未執行\"], value=\"尚未執行\"), empty_df\n",
        "\n",
        "    # --- 參數驗證 ---\n",
        "    try:\n",
        "        top_n = int(top_n_str)\n",
        "        pages_to_fetch = int(pages_to_fetch_str)\n",
        "        if top_n <= 0 or pages_to_fetch <= 0:\n",
        "            log_output.append(\"❌ Top N 或爬取頁數必須是大於 0 的數字。\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_df\n",
        "            return\n",
        "    except ValueError:\n",
        "        log_output.append(\"❌ 請輸入有效的數字。\")\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_df\n",
        "        return\n",
        "\n",
        "    # --- 自動化流程 ---\n",
        "    log_output.append(\"===================================================\")\n",
        "    log_output.append(f\"🚀 自動化流程啟動 ({datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\")\n",
        "    log_output.append(\"===================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- 步驟 1: 爬蟲 ---\n",
        "        log_output.append(\"1/4: 🏃‍♂️ 開始爬取 PTT JapanDrama 版文章...\")\n",
        "        # [修正 Yield 1] 補上第 5 個值 empty_df\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), empty_df\n",
        "\n",
        "        scraped_df, log_output = scrape_ptt_japandrama(pages_to_fetch, log_output)\n",
        "        # [修正 Yield 2] scraped_df 已更新\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        if scraped_df.empty:\n",
        "            log_output.append(\"❌ 爬蟲失敗，未抓取到任何資料。流程終止。\")\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), empty_df\n",
        "            return\n",
        "\n",
        "        # --- 步驟 2: 寫入 Sheet (可選) ---\n",
        "        log_output = write_to_sheet(gsheets, \"PTT文章列表\", scraped_df, log_output)\n",
        "\n",
        "        # --- 步驟 3: TF-IDF 分析 ---\n",
        "        log_output.append(\"2/4: 📊 正在進行 Sklearn TF-IDF 關鍵字分析...\")\n",
        "        # [修正 Yield 3] 補上第 5 個值 scraped_df (因為爬蟲列表應繼續顯示)\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        keywords_df, log_output = get_tfidf_keywords(scraped_df, top_n, log_output)\n",
        "\n",
        "        plot_df = None # 先宣告\n",
        "        if not keywords_df.empty:\n",
        "          # 為了讓 BarPlot 頂部顯示權重最高的，我們需將 df 升冪排序\n",
        "          plot_df = keywords_df.sort_values(\"TF-IDF平均權重\", ascending=True)\n",
        "\n",
        "        # [修正 Yield 4] 第 2 個值更新為 keywords_df\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        if keywords_df.empty:\n",
        "            log_output.append(\"⚠️ 分析完成，但未提取到關鍵字。流程終止。\")\n",
        "            # [修正 Yield 5] 第 2 個值是 empty_df (正確)，但第 5 個值 scraped_df 應保留\n",
        "            yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list), scraped_df\n",
        "            return\n",
        "\n",
        "        # --- 步驟 4: 寫入 Sheet (可選) ---\n",
        "        log_output.append(\"3/4: 📈 正在將 Top 熱詞回寫至 Sheet (熱詞統計)...\")\n",
        "        log_output = write_to_sheet(gsheets, \"熱詞統計\", keywords_df, log_output)\n",
        "\n",
        "        # --- 步驟 5: Gemini 摘要 ---\n",
        "        log_output.append(\"4/4: 🧠 正在呼叫 Gemini API 生成摘要...\")\n",
        "        # [修正 Yield 6] 第 2 個值是 keywords_df (應保留)，補上第 5 個值 scraped_df\n",
        "        yield \"\\n\".join(log_output), keywords_df, empty_str, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        summary, log_output = get_gemini_summary(keywords_df, log_output)\n",
        "      \t# [修正 Yield 7] 第 2 個值是 keywords_df，第 3 個值更新為 summary\n",
        "        yield \"\\n\".join(log_output), keywords_df, summary, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "        log_output.append(\"===================================================\")\n",
        "        log_output.append(\"✅ 全部流程完成！請切換到「最終結果」標籤頁查看。\")\n",
        "\n",
        "        # 最終回傳 (您原本這行就是對的)\n",
        "        yield \"\\n\".join(log_output), keywords_df, summary, plot_df, gr.Radio(choices=site_list, value=site_list[0]), scraped_df\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ 流程發生未預期錯誤：{e}\\n{traceback.format_exc()}\"\n",
        "        log_output.append(error_msg)\n",
        "      \t# [修正 Yield 8] 確保錯誤回傳也是 5 個值\n",
        "        yield \"\\n\".join(log_output), empty_df, empty_str, None, gr.Radio(choices=site_list, value=site_list[0]), empty_df"
      ],
      "metadata": {
        "id": "XNcmZqtMBHVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5. 啟動 Gradio 介面 (已升級 Tab 佈局)\n",
        "# ================================\n",
        "print(\"\\n🚀 正在啟動 Gradio 介面...\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # 📊 PTT 熱詞分析與摘要系統\n",
        "\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Tab(\"🚀 自動化流程執行\"):\n",
        "        with gr.Row():\n",
        "            pages_to_fetch_input = gr.Textbox(label=\"要爬取的頁數\", value=\"10\", scale=1)\n",
        "            top_n_input = gr.Textbox(label=\"要統計的 Top N 熱詞數量\", value=\"20\", scale=1)\n",
        "            run_btn = gr.Button(\"🚀 一鍵啟動自動化流程\", variant=\"primary\", scale=2)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        # 使用 Tab 來區分最終結果和日誌\n",
        "        with gr.Tabs():\n",
        "\n",
        "            with gr.TabItem(\"🛠️ 技術日誌與輸出細節\"):\n",
        "                # 使用 Textbox 來顯示詳細的日誌\n",
        "                log_output_text = gr.Textbox(\n",
        "                    label=\"詳細流程日誌 (爬蟲、寫入、分析步驟)\",\n",
        "                    lines=30,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "            with gr.TabItem(\"🕸️ 爬取之網站\"):\n",
        "                site_list_output = gr.Radio(\n",
        "                    label=\"資料來源\",\n",
        "                    choices=[\"尚未執行\"],\n",
        "                    value=\"尚未執行\",\n",
        "                    interactive=False\n",
        "                )\n",
        "                gr.Markdown(\"---\")\n",
        "\n",
        "                link_display_output = gr.Markdown(\n",
        "          \t\t\tvalue=\"*點擊上方表格中的任一文章，連結將顯示於此*\"\n",
        "          \t\t  )\n",
        "\n",
        "                scraped_data_output = gr.Dataframe(\n",
        "              \t\tlabel=\"爬取文章列表 (原始資料)\",\n",
        "              \t\theaders=[\"日期\", \"作者\", \"標題\", \"連結\"], # 預先定義欄位\n",
        "              \t\tinteractive=True,\n",
        "              \t\trow_count=(15, 'dynamic')\n",
        "              \t)\n",
        "\n",
        "\n",
        "\n",
        "            with gr.TabItem(\"✅ 最終結果\"):\n",
        "                summary_output = gr.Markdown(label=\"🤖 Gemini 洞察摘要與結論\")\n",
        "\n",
        "                keyword_plot_output = gr.BarPlot(\n",
        "                  label=\"📈 Top N 熱詞視覺化圖表\",\n",
        "                  x=\"TF-IDF平均權重\", # X 軸 (值)\n",
        "                  y=\"關鍵字\",       # Y 軸 (類別)\n",
        "                  tooltip=['關鍵字', 'TF-IDF平均權重'],\n",
        "                  color=\"TF-IDF平均權重\",\n",
        "                  vertical=False, # 設為水平長條圖\n",
        "                  height=400\n",
        "                )\n",
        "\n",
        "\n",
        "                keywords_output = gr.Dataframe(label=\"📈 Top N 熱詞統計結果 (Sklearn TF-IDF 平均權重)\")\n",
        "\n",
        "\n",
        "        # 設定點擊事件\n",
        "        run_btn.click(\n",
        "          fn=run_full_automation_flow,\n",
        "          \tinputs=[top_n_input, pages_to_fetch_input],\n",
        "          \toutputs=[\n",
        "                log_output_text,\n",
        "                keywords_output,\n",
        "                summary_output,      # 3. 摘要 (Markdown)\n",
        "      \t\t      keyword_plot_output,\n",
        "                site_list_output,\n",
        "                scraped_data_output\n",
        "          \t]\n",
        "        )\n",
        "\n",
        "        scraped_data_output.select(\n",
        "      \t\tfn=show_selected_link,\n",
        "      \t\tinputs=[scraped_data_output], # 將 Dataframe 本身作為輸入\n",
        "      \t\toutputs=[link_display_output], # 將連結輸出到 Markdown\n",
        "      \t\tshow_progress=False # 點擊時不需要顯示 \"loading\"\n",
        "    \t  )\n",
        "\n",
        "    with gr.Tab(\"🔬 Jieba 分詞模式測試\"):\n",
        "        gr.Markdown(\"### 中文分詞模式比較\")\n",
        "        jieba_input = gr.Textbox(\n",
        "            label=\"輸入中文句子 (例如: 日本電視台與製作公司合作推出驚悚新劇)\",\n",
        "            value=\"日本電視台與製作公司合作推出驚悚新劇\",\n",
        "            lines=2\n",
        "        )\n",
        "\n",
        "        jieba_run_btn = gr.Button(\"🔍 執行分析\")\n",
        "\n",
        "        # 使用 Row 來水平展示四種模式\n",
        "        with gr.Row():\n",
        "            precise_output = gr.Textbox(label=\"1. 精確模式 (最常用)\")\n",
        "            all_output = gr.Textbox(label=\"2. 全模式 (所有可能組合)\")\n",
        "        with gr.Row():\n",
        "            search_output = gr.Textbox(label=\"3. 搜尋引擎模式 (長詞再切)\")\n",
        "            pos_output = gr.Textbox(label=\"4. 詞性標註 (詞語/詞性)\")\n",
        "\n",
        "        jieba_run_btn.click(\n",
        "            fn=demonstrate_jieba_modes,\n",
        "            inputs=[jieba_input],\n",
        "            outputs=[precise_output, all_output, search_output, pos_output],\n",
        "        )\n",
        "\n",
        "\n",
        "demo.queue().launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "kd6_uccnCdmD",
        "outputId": "8f231987-77c7-40d5-aeed-24312fce9c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 正在啟動 Gradio 介面...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://db54f8bc2c74d9575d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://db54f8bc2c74d9575d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://db54f8bc2c74d9575d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}